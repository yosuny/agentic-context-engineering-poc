# ACE 프로젝트 전략적 가치 제언 (Team FAQ)

팀원들의 날카로운 질의에 대한 답변과, 이를 증명하기 위해 프로젝트에서 검증해야 할 핵심 포인트를 정리했습니다.

---

## Q1. 도메인 전문가들이 데이터셋을 만든다면 RLHF나 SFT와 차이는 무엇일까요?

**핵심 차별점: "지식의 가소성(Plasticity)"과 "설명 가능성(Explainability)"**

도메인 전문가가 개입한다는 점은 동일하지만, **"전문가의 지식이 어디에, 어떻게 저장되는가?"**가 가장 큰 차이입니다.

| 비교 항목 | **SFT / RLHF (Fine-tuning)** | **ACE (Context Engineering)** |
| :--- | :--- | :--- |
| **지식 저장소** | **모델 가중치(Weights)** (Black Box) | **플레이북(Playbook)** (White Box) |
| **업데이트 비용** | 높음 (GPU 학습, 수 시간~수 일 소요) | **매우 낮음** (텍스트 파일 수정, 즉시 반영) |
| **데이터 요구량** | 수천~수만 건 (Low-Quality도 다수 필요) | **수십 건** (High-Quality 소수 정예) |
| **전문가의 역할** | 데이터 라벨러 (정답지 채점) | **전략가/멘토** (노하우 전수, 룰 제정) |
| **치명적 오류 수정** | 재학습 필요 (원인 파악 어려움) | **해당 룰 삭제/수정으로 즉시 해결** |

### 🛠️ 프로젝트 검증 포인트 (Derived Verification Setup)
이 차이를 증명하기 위해 다음 내용을 검증 항목에 포함해야 합니다.
1.  **Cold Start 성능**: 사전 학습 없이도(Weight 수정 없이) 짧은 시간 안에 성능이 올라가는가? (이미 0% -> 20%로 증명 중)
2.  **Rule 수정의 즉시성**: 전문가가 플레이북의 특정 룰을 수동으로 고쳤을 때, 다음 추론에 바로 반영되는가?
3.  **데이터 효율성**: 수천 건이 아닌, 단 20~30건의 사례만으로도 도메인 적응이 가능한가?

---

## Q2. RAG는 단순 검색 같고, Palantir는 프로세스 개선 같은데, ACE의 강점(Killer Feature)은 무엇인가요?

**핵심 정의: ACE는 "살아있는 표준 운영 절차(Dynamic SOP Engine)" 입니다.**

RAG와 ACE는 다루는 **지식의 층위**가 다릅니다.
*   **RAG (검색)**: "매뉴얼 몇 페이지에 이 내용이 있나요?" (Fact Retrieval)
*   **ACE (판단/컨설팅)**: "이 상황에서 김 대리가 실수한 게 무언인가요?" (Reasoning & Strategy)

ACE는 단순히 문서를 찾아주는 것이 아니라, 전문가의 **"암묵지(Tacit Knowledge)"를 "형식지(Explicit Knowledge)"로 변환**하여 지속적으로 축적하는 시스템입니다.

### 🆚 비교 분석
1.  **vs RAG**: RAG는 문서를 찾지만, 문맥(Context)을 이해하고 적용하진 못합니다. ACE는 **"상황(Context)에 맞는 룰(Rule)을 적용"**합니다.
2.  **vs Palantir**: Palantir가 로그(Log) 기반의 프로세스 마이닝이라면, ACE는 **텍스트(Text/Event)** 기반의 **"로직(Logic) 마이닝"**입니다. 사고 경위서나 상담 로그 같은 비정형 데이터에서 실패 원인을 찾아냅니다.

### 🎯 ACE의 강점이 드러나는 타겟 시나리오
단순 지식 검색이 아니라, **"복합적인 상황 판단"**이 필요한 영역을 타겟팅해야 합니다.

*   **🟢 Best Case (ACE 강점)**:
    *   **사고 원인 조사**: "분명 매뉴얼대로 했는데 사고가 났다. 원인이 뭐지?" -> (ACE: "매뉴얼에는 없지만, 관행적으로 무시되던 '배관 퍼지' 절차가 누락되었습니다.")
    *   **불완전한 정보 하의 의사결정**: "고객 정보가 부족한데 대출 승인해도 될까?" -> (ACE: "이런 패턴의 경우 금융사기일 확률이 높으니 보류하십시오.")

*   **🔴 Worst Case (RAG가 더 나음)**:
    *   단순 법조항 검색 ("산업안전보건법 제3조 내용 보여줘")
    *   스펙 조회 ("HCX-007 모델의 파라미터 수는?")

### 🛠️ 프로젝트 검증 포인트 (Derived Verification Setup)
ACE의 강점을 부각하기 위해 평가 데이터셋을 다음과 같이 구성/검증해야 합니다.
1.  **함정(Trap) 문제 포함**: 겉보기엔 정상 같지만, 숨겨진 룰을 어긴 시나리오를 테스트합니다. (단순 검색으로는 풀 수 없는 문제)
2.  **Playbook의 구체성 평가**: 플레이북에 "00법 0조 참조" 같은 인덱스가 아니라, **"~할 때는 반드시 ~를 확인하라"**는 구체적인 행동 지침(Actionable Insight)이 쌓이는지 확인합니다.

---

## Q3. 모델 튜닝도 아닌데 왜 "학습(Learning)"인가요? "Epoch"는 무슨 의미인가요?

ACE 프레임워크에서 사용하는 용어는 기존 딥러닝 용어와 맥락이 약간 다릅니다.

### 1. 학습 (Learning) = Context Optimization
*   **Deep Learning**: 모델의 **파라미터(Weight)**를 수정하여 지식을 저장합니다.
*   **ACE Learning**: 모델이 참조하는 **문맥(Context/Playbook)**을 수정하여 지식을 저장합니다.
    *   마치 사람에게 "매뉴얼(Playbook)"을 쥐어주고, 일을 시킨 뒤(Generation), "너 이거 틀렸어, 매뉴얼 고쳐(Reflection/Curation)"라고 피드백을 주는 과정과 같습니다. 이를 **In-Context Learning (ICL)의 최적화 과정**으로 정의합니다.

### 2. Epoch = Playbook Refinement Cycle
*   **Deep Learning Epoch**: 전체 데이터를 모델에 한 번 통과시키는 것.
*   **ACE Epoch**: 준비된 시나리오 세트 전체를 한 번 경험하게 하는 것.
    *   **Epoch 1**: 아무것도 모르는 상태에서 겪으며 플레이북 초안 생성. (실패를 통한 학습)
    *   **Epoch 2~N**: 생성된 플레이북을 들고 다시 같은(혹은 유사한) 문제를 풀며, **잘못된 규칙을 수정하고 부족한 규칙을 보완**하는 과정.
    *   ACE에서는 Epoch가 반복될수록 플레이북의 **정교함(Precision)**이 올라가고 **일반화 성능**이 개선됩니다.

---

## Q4. 실제 서비스에서도 유저가 "카테고리"를 선택해야 하나요?

아닙니다. 평가용 데이터셋의 `category` 필드는 **오직 평가 지표 산출용**입니다.

1.  **Production Environment (실제 서비스)**
    *   **Input**: 유저의 자연어 질문 (Raw Text)만 입력받습니다.
    *   **Process**: ACE 모델(Generator)이 질문의 맥락을 스스로 파악하여 플레이북의 적절한 규칙을 찾아냅니다. 유저가 "이건 지게차 문제입니다"라고 입력할 필요가 없습니다.

2.  **Evaluation Dataset (평가용 데이터)**
    *   **Role**: 테스트 스크립트가 "모델이 지게차 문제를 얼마나 잘 맞췄는지" 채점하기 위해 `category` 메타데이터를 가지고 있습니다.
    *   **Method**: 실제 모델에게는 이 `category` 정보를 주지 않고 질문(`question`)만 던집니다. 즉, 모델은 스스로 카테고리를 추론해야 하는 실전과 동일한 환경에서 테스트받습니다.
